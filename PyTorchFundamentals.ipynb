{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOSLor1DNorFqMKWF4KRH8N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JWiseman-git/ml_sandbox/blob/main/PyTorchFundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the work through described in this documentation https://www.learnpytorch.io/ for a comprehensive refresh in pytorch.\n",
        "\n"
      ],
      "metadata": {
        "id": "ANeIv3a0LbJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fundamentals"
      ],
      "metadata": {
        "id": "7zDRkH7eLS4J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWPx1TVC_Y6k"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor - i x j x k - $T_{ijk}$\n",
        "\n",
        "Reductive - higher order matrics\n",
        "\n",
        "independent of underlying coordinate system\n"
      ],
      "metadata": {
        "id": "RERDZjQbDlU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scalar = torch.tensor(3)\n",
        "scalar.ndim"
      ],
      "metadata": {
        "id": "bJ_GqSgtGXl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = torch.tensor([1,2])\n",
        "vector.ndim"
      ],
      "metadata": {
        "id": "_PhBiZ7lGxrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = torch.tensor([[[1, 2, 3],\n",
        "                        [3, 6, 9],\n",
        "                        [2, 4, 5]]])\n",
        "matrix.shape\n",
        "\n",
        "# torch.size([0,1,2]) -> first, second and third dims"
      ],
      "metadata": {
        "id": "mDFGmhRCG27b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_tensor = torch.rand(3, 4)\n",
        "random_tensor"
      ],
      "metadata": {
        "id": "h-8l4brJH7Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random tensor of size (224, 224, 3)\n",
        "random_image_size_tensor = torch.rand(size=(224, 224, 3))\n",
        "random_image_size_tensor.shape, random_image_size_tensor.ndim"
      ],
      "metadata": {
        "id": "G7e4S6DjIZWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch zeros - useful for masking purposes\n",
        "zeros = torch.zeros(size=(3, 4))\n",
        "zeros, zeros.dtype"
      ],
      "metadata": {
        "id": "f-JvFC-ZIkxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Floating point value used to indicate the precision to which a value is calculated - influencing the amount of data used to represent it\n",
        "- Tensors are initialised with a specific device allocated to specify the hardware for memory.\n",
        "- VRAM used by GPU and RAM used by CPU\n",
        "- Specifying device at the start of a script:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import torch\n",
        "\n",
        "# 1. Check for GPU (CUDA) or Apple Silicon (MPS)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\") # For Mac M1/M2/M3 chips\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 2. Use the 'device' variable for all future initializations\n",
        "data = torch.ones((3, 3), device=device)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "EDUCX8qJKYDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
        "                               dtype=torch.float16) # torch.half would also work\n",
        "\n",
        "float_16_tensor.dtype"
      ],
      "metadata": {
        "id": "Q5y-B-XgK8Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Operations\n",
        "\n",
        "- inner dimensions of a matrix must match to allow for multiplication\n",
        "- for example (3, 2) @ (2, 3)\n",
        "\n",
        "The Dimensional Rule\n",
        "\n",
        "If Matrix $A$ has dimensions $(m \\times n)$ and Matrix $B$ has dimensions $(p \\times q)$, they can only be multiplied if:$$n = p$$The resulting matrix will have the dimensions $(m \\times q)$.\n",
        "\n",
        "When are Transposed Matrices used?\n",
        "\n",
        "While it isn't a\n",
        " requirement, there are specific scenarios where you multiply a matrix by its transpose ($A \\times A^T$ or $A^T \\times A$):"
      ],
      "metadata": {
        "id": "toxE5aWWLmWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#matric multiplication\n",
        "\n",
        "torch = torch.matmul(torch.rand(3, 2), torch.rand(2, 3))\n",
        "torch"
      ],
      "metadata": {
        "id": "rIrw3SgsNHG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Shapes need to be in the right way\n",
        "tensor_A = torch.tensor([[1, 2],\n",
        "                         [3, 4],\n",
        "                         [5, 6]], dtype=torch.float32)\n",
        "\n",
        "tensor_B = torch.tensor([[7, 10],\n",
        "                         [8, 11],\n",
        "                         [9, 12]], dtype=torch.float32)\n",
        "\n",
        "# The operation works when tensor_B is transposed\n",
        "print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\n",
        "print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\n",
        "print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\n",
        "print(\"Output:\\n\")\n",
        "output = torch.matmul(tensor_A, tensor_B.T)\n",
        "print(output)\n",
        "print(f\"\\nOutput shape: {output.shape}\")"
      ],
      "metadata": {
        "id": "Oa1Wu3iUR4yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.mm is a shortcut for matmul\n",
        "torch.mm(tensor_A, tensor_B.T)"
      ],
      "metadata": {
        "id": "BK0rR5hhSPtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feed-forward layer implements several matrix multiplication steps\n",
        "- Fully connected/dense layer - every neuron connects to every neuron in the previous layer"
      ],
      "metadata": {
        "id": "Nfa7vb7MS64G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(42)\n",
        "# randomly seed the weights of the matrix\n",
        "# implements a matrix multiplication between an input x and a Weights matrix A\n",
        "linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input\n",
        "                         out_features=6) # out_features = describes outer value\n",
        "x = tensor_A\n",
        "output = linear(x)\n",
        "print(f\"Input shape: {x.shape}\\n\")\n",
        "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"
      ],
      "metadata": {
        "id": "knlc9fLcS_zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.arange(1., 8.)\n",
        "x_reshaped = x.reshape(1, 7)\n",
        "x_stacked = torch.stack([x, x, x, x], dim=1)\n",
        "x_stacked"
      ],
      "metadata": {
        "id": "gUW9sQWeHEDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view - returns a view of the original tensor but in a different shape\n",
        "\n",
        "# Create tensor with specific shape\n",
        "x_original = torch.rand(size=(224, 224, 3))\n",
        "\n",
        "# Permute the original tensor to rearrange the axis order\n",
        "x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n",
        "\n",
        "print(f\"Previous shape: {x_original.shape}\")\n",
        "print(f\"New shape: {x_permuted.shape}\")"
      ],
      "metadata": {
        "id": "2bSK7a0uI7jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch and numpy\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "array = np.arange(1.0, 8.0)\n",
        "tensor = torch.from_numpy(array)\n",
        "array, tensor\n"
      ],
      "metadata": {
        "id": "UeOugqsJJvnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trialing use of GPU\n",
        "\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.device_count()\n",
        "\n",
        "# Create tensor (default on CPU)\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "\n",
        "# Tensor not on GPU\n",
        "print(tensor, tensor.device)\n",
        "\n",
        "# Move tensor to GPU (if available)\n",
        "tensor_on_gpu = tensor.to(device)\n",
        "tensor_on_gpu"
      ],
      "metadata": {
        "id": "JOBVnlMRMvD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow Fundamentals\n"
      ],
      "metadata": {
        "id": "JsiB09aCJPN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create *known* parameters\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "# Create data\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "# unsqueeze adds an additional dimension\n",
        "\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y = weight * X + bias\n",
        "\n",
        "X[:10], y[:10]\n"
      ],
      "metadata": {
        "id": "6YW7WczQJVEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- training set - the data the models learns from\n",
        "- validation set - the tuning data set (confirming behaivour)\n",
        "- testing set - model evaluated on this data"
      ],
      "metadata": {
        "id": "MG7h4m9-Kfyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train/test split\n",
        "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "id": "xOgJtGz_Mf8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(train_data=X_train,\n",
        "                     train_labels=y_train,\n",
        "                     test_data=X_test,\n",
        "                     test_labels=y_test,\n",
        "                     predictions=None):\n",
        "  \"\"\"\n",
        "  Plots training data, test data and compares predictions.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10, 7))\n",
        "\n",
        "  # Plot training data in blue\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
        "\n",
        "  # Plot test data in green\n",
        "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
        "\n",
        "  if predictions is not None:\n",
        "    # Plot the predictions in red (predictions were made on the test data)\n",
        "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "  # Show the legend\n",
        "  plt.legend(prop={\"size\": 14});\n",
        "plot_predictions()"
      ],
      "metadata": {
        "id": "OnXikyGSM5Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Linear Regression model class\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(1,dtype=torch.float),requires_grad=True)\n",
        "        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)\n",
        "                                            dtype=torch.float), # <- PyTorch loves float32 by default\n",
        "                                requires_grad=True) # <- can we update this value with gradient descent?))\n",
        "\n",
        "    # Forward defines the computation in the model - always needed\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
        "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)"
      ],
      "metadata": {
        "id": "JXhPUKntNFfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   autograd - requirs_grad = True\n",
        "*   torch.nn.parameter - stores tensors that can be used with the nn.module\n",
        "\n"
      ],
      "metadata": {
        "id": "6m7zE4FEN26b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_0 = LinearRegressionModel()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    y_preds = model_0(X_test)\n",
        "\n",
        "print(f\"Number of testing samples: {len(X_test)}\")\n",
        "print(f\"Number of predictions made: {len(y_preds)}\")\n",
        "print(f\"Predicted values:\\n{y_preds}\")\n",
        "\n",
        "plot_predictions(predictions=y_preds)"
      ],
      "metadata": {
        "id": "MTdUsz8aOKeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. loss functions\n",
        "- mean absolute error - MAE - for regression problems. Measures the absolute difference between two points.\n",
        "- binary cross entropy for binary classification problems\n",
        "\n",
        "2. Optimizers\n",
        "- stochastic gradient descent - best when predicting numbers\n",
        "- adam optimizer - best when predicting one thing or another\n",
        "- zero gradients option - `optimizer.zero_grad()` - recalculated for each specific training step\n",
        "\n",
        "3. Learning Rate\n",
        "- hyperparameter - influence the size of the updates during optimization\n",
        "- learning rate schedule - how the learning rate is adjusted over time\n",
        "\n",
        "4. training and testing loops\n",
        "- training loop - model goes through testing data and evaluates patterns\n",
        "- testing loop - evaluating how good the parameters are the model learned on the training data\n",
        "----\n",
        "\n",
        "Training loop:\n",
        "1. Pass the data through the model for a number of epochs - 100 epochs would mean 100 passes\n",
        "2. foward pass\n",
        "3. for each forward pass caculate loss value\n",
        "4. zero optimizer gradients\n",
        "5. backpropagation step - gradient of every parameter\n",
        "6. step the optimiser - update the model's parameters with respect to the gradients calculated\n",
        "\n",
        "----\n",
        "Testing Loop\n",
        "1. Forward Pass\n",
        "2. inference mode only - no need for gradient tracking"
      ],
      "metadata": {
        "id": "rNoAFBJZXvZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n",
        "                            lr=0.01)\n"
      ],
      "metadata": {
        "id": "16mTdG-JaSdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs (how many times the model will pass over the training data)\n",
        "epochs = 500\n",
        "\n",
        "# Create empty loss lists to track values\n",
        "train_loss_values = []\n",
        "test_loss_values = []\n",
        "epoch_count = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "\n",
        "    # Put model in training mode (this is the default state of a model)\n",
        "    model_0.train()\n",
        "\n",
        "    # 1. Forward pass on train data using the forward() method inside\n",
        "    y_pred = model_0(X_train)\n",
        "    # print(y_pred)\n",
        "\n",
        "    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "    # 3. Zero grad of the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Progress the optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "\n",
        "    # Put the model in evaluation mode\n",
        "    model_0.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "      # 1. Forward pass on test data\n",
        "      test_pred = model_0(X_test)\n",
        "\n",
        "      # 2. Caculate loss on test data\n",
        "      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
        "\n",
        "      # Print out what's happening\n",
        "      if epoch % 10 == 0:\n",
        "            epoch_count.append(epoch)\n",
        "            train_loss_values.append(loss.detach().numpy())\n",
        "            test_loss_values.append(test_loss.detach().numpy())\n",
        "            # print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"
      ],
      "metadata": {
        "id": "8MJoRa7Be7ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves\n",
        "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
        "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "jDIIgZhwfQP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The model learned the following values for weights and bias:\")\n",
        "print(model_0.state_dict())\n",
        "print(\"\\nAnd the original values for weights and bias are:\")\n",
        "print(f\"weights: {weight}, bias: {bias}\")"
      ],
      "metadata": {
        "id": "crgdUEEqf3oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inference Basics\n",
        "\n",
        "1. Set the model in evaluation mode - `model.eval()`\n",
        "2. inference mode context manager\n",
        "3. predictions should be made with objects on the same device"
      ],
      "metadata": {
        "id": "QexKgs47gFF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_0(X_test)\n",
        "y_preds"
      ],
      "metadata": {
        "id": "u8a8RASqgOfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions=y_preds)"
      ],
      "metadata": {
        "id": "vn9Gvzbzi9hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving and Loading a Pytorch Model\n",
        "\n",
        "- `torch.save` - save a model as a pickle file\n",
        "- `torch.load` - load a model which was saved as  pickle file\n",
        "- `torch.nn.Module.load_state_dict`- load a model's parameter dictionary using a `saved_dict()` object"
      ],
      "metadata": {
        "id": "1NmxX-7CjIu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model saving\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Create models directory\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Create model save path\n",
        "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# 3. Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "JhVICva202u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model loading\n",
        "# Instantiate a new instance of our model (this will be instantiated with random weights)\n",
        "loaded_model_0 = LinearRegressionModel()\n",
        "\n",
        "# Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\n",
        "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "# 1. Put the loaded model into evaluation mode\n",
        "loaded_model_0.eval()\n",
        "\n",
        "# 2. Use the inference mode context manager to make predictions\n",
        "with torch.inference_mode():\n",
        "    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model"
      ],
      "metadata": {
        "id": "_4FcjpUC1Tde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Full Example\n",
        "\n",
        "1. Forward pass - The model goes through all of the training data once, performing its forward() function calculations (model(x_train)).\n",
        "2. Calculate the loss - The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are (loss = loss_fn(y_pred, y_train).\n",
        "3. Zero gradients - The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step (optimizer.zero_grad()).\n",
        "4. Perform backpropagation on the loss - Computes the gradient of the loss with respect for every model parameter to be updated (each parameter with requires_grad=True). This is known as backpropagation, hence \"backwards\" (loss.backward()).\n",
        "5. Step the optimizer (gradient descent) - Update the parameters with requires_grad=True with respect to the loss gradients in order to improve them (optimizer.step())."
      ],
      "metadata": {
        "id": "qnZNvrTG1rVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "# Create weight and bias\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "# Create range values\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "\n",
        "# Create X and y (features and labels)\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\n",
        "y = weight * X + bias\n",
        "X[:10], y[:10]\n",
        "\n",
        "\n",
        "# Split data\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)\n",
        "\n",
        "\n",
        "# Subclass nn.Module to make our model\n",
        "class LinearRegressionModelV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Use nn.Linear() for creating the model parameters\n",
        "        self.linear_layer = nn.Linear(in_features=1,\n",
        "                                      out_features=1)\n",
        "\n",
        "    # Define the forward computation (input data x flows through nn.Linear())\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear_layer(x)\n",
        "\n",
        "# Set the manual seed when creating the model (this isn't always needed but is used for demonstrative purposes, try commenting it out and seeing what happens)\n",
        "torch.manual_seed(42)\n",
        "model_1 = LinearRegressionModelV2()\n",
        "model_1, model_1.state_dict()\n",
        "\n",
        "model_1.to(device)\n",
        "next(model_1.parameters()).device\n",
        "\n",
        "loss_fn = nn.L1Loss()\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n",
        "                            lr=0.01)\n"
      ],
      "metadata": {
        "id": "j4UO6eqS1ush"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 1000\n",
        "\n",
        "# Put data on the available device\n",
        "# Without this, error will happen (not all model/data on device)\n",
        "X_train = X_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "y_train = y_train.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    model_1.train() # train mode is on by default after construction\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_pred = model_1(X_train)\n",
        "\n",
        "    # 2. Calculate loss\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "    # 3. Zero grad optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Step the optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    model_1.eval() # put the model in evaluation mode for testing (inference)\n",
        "    # 1. Forward pass\n",
        "    with torch.inference_mode():\n",
        "        test_pred = model_1(X_test)\n",
        "\n",
        "        # 2. Calculate the loss\n",
        "        test_loss = loss_fn(test_pred, y_test)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")"
      ],
      "metadata": {
        "id": "SgbjebaI3DZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(model_1.state_dict())"
      ],
      "metadata": {
        "id": "-lZH02Bc34zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn model into evaluation mode\n",
        "model_1.eval()\n",
        "\n",
        "# Make predictions on the test data\n",
        "with torch.inference_mode():\n",
        "    y_preds = model_1(X_test)\n",
        "y_preds"
      ],
      "metadata": {
        "id": "QB3Z2jcc3_9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# 1. Create models directory\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Create model save path\n",
        "MODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# 3. Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "sRmGWp-w4DDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a fresh instance of LinearRegressionModelV2\n",
        "loaded_model_1 = LinearRegressionModelV2()\n",
        "\n",
        "# Load model state dict\n",
        "loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\n",
        "loaded_model_1.to(device)\n",
        "\n",
        "print(f\"Loaded model:\\n{loaded_model_1}\")\n",
        "print(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")"
      ],
      "metadata": {
        "id": "AR5S9PDR4F63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Examples\n",
        "\n",
        "- implementing a basic binary classification network\n",
        "- supervised learning example"
      ],
      "metadata": {
        "id": "k_PW4y2vKOVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "n_samples = 1000\n",
        "X, y = make_circles(n_samples,\n",
        "                    noise=0.03,\n",
        "                    random_state=42)\n",
        "\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y = torch.from_numpy(y).type(torch.float)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2, # 20% test, 80% train\n",
        "                                                    random_state=42) # make the random split reproducible\n",
        "\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "S5csmytBKUgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Construct a model class that subclasses nn.Module\n",
        "class CircleModelV0(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes\n",
        "        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n",
        "        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n",
        "\n",
        "    # 3. Define a forward method containing the forward pass computation\n",
        "    def forward(self, x):\n",
        "        # Return the output of layer_2, a single feature, the same shape as y\n",
        "        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n",
        "\n",
        "# 4. Create an instance of the model and send it to target device\n",
        "model_0 = CircleModelV0().to(device)\n",
        "model_0 = nn.Sequential(\n",
        "    nn.Linear(in_features=2, out_features=5),\n",
        "    nn.Linear(in_features=5, out_features=1)\n",
        ").to(device)\n",
        "\n",
        "model_0"
      ],
      "metadata": {
        "id": "sS99zFPz8Tgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "untrained_preds = model_0(X_test.to(device))\n",
        "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
        "print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\n",
        "print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\n",
        "print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")\n"
      ],
      "metadata": {
        "id": "SoXMmCvO9AaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different types of loss functions/optimizers:\n",
        "\n",
        "- SGD Optimizer - Classification, regression, many others\n",
        "- Adam Optimizer - Classification, regression, many others\n",
        "- Binary Cross Entropy Loss - Binary Classification\n",
        "- Cross entropy loss - multi-class classification"
      ],
      "metadata": {
        "id": "QHODE219BW9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCEWithLogitsLoss()  # inbuilt sigmoid layer nn.Sigmoid\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "12rciKRBC7-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ],
      "metadata": {
        "id": "rqSql3_9DFxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logits > prediction probabilities\n",
        "\n",
        "# View the frist 5 outputs of the forward pass on the test data\n",
        "y_logits = model_0(X_test.to(device))[:5]\n",
        "y_pred_probs = torch.sigmoid(y_logits)\n",
        "# Find the predicted labels (round the prediction probabilities)\n",
        "y_preds = torch.round(y_pred_probs)\n",
        "\n",
        "# In full\n",
        "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
        "\n",
        "# Check for equality\n",
        "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
        "\n",
        "# Get rid of extra dimension\n",
        "y_preds.squeeze()"
      ],
      "metadata": {
        "id": "-dZjkCfS5Z52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 100\n",
        "\n",
        "# Put data to target device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "# Build training and evaluation loop\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    model_0.train()\n",
        "\n",
        "    # 1. Forward pass (model outputs raw logits)\n",
        "    y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device\n",
        "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labls\n",
        "\n",
        "    # 2. Calculate loss/accuracy\n",
        "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
        "    #                y_train)\n",
        "    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n",
        "                   y_train)\n",
        "    acc = accuracy_fn(y_true=y_train,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "        # 1. Forward pass\n",
        "        test_logits = model_0(X_test).squeeze()\n",
        "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "        # 2. Caculate loss/accuracy\n",
        "        test_loss = loss_fn(test_logits,\n",
        "                            y_test)\n",
        "        test_acc = accuracy_fn(y_true=y_test,\n",
        "                               y_pred=test_pred)\n",
        "\n",
        "    # Print out what's happening every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "0usmhJ7p6QFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n",
        "from helper_functions import plot_predictions, plot_decision_boundary"
      ],
      "metadata": {
        "id": "7TZjuwAg7zND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundaries for training and test sets\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_0, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_0, X_test, y_test)"
      ],
      "metadata": {
        "id": "NWYBEKK778qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### improving a model - hyperparameters\n",
        "\n",
        "- make the model deeper (more layers)\n",
        "- add more hidden units\n",
        "- Fitting for longer (more epochs)\n",
        "- Changing the activation functions\n",
        "- Change the learning rate\n",
        "- Change the loss function\n",
        "- Use transfer learning"
      ],
      "metadata": {
        "id": "g0bdj19d8Es-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CircleModelV1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
        "        self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer\n",
        "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
        "\n",
        "    def forward(self, x): # note: always make sure forward is spelt correctly!\n",
        "        # Creating a model like this is the same as below, though below\n",
        "        # generally benefits from speedups where possible.\n",
        "        # z = self.layer_1(x)\n",
        "        # z = self.layer_2(z)\n",
        "        # z = self.layer_3(z)\n",
        "        # return z\n",
        "        return self.layer_3(self.layer_2(self.layer_1(x)))\n",
        "\n",
        "model_1 = CircleModelV1().to(device)\n",
        "model_1"
      ],
      "metadata": {
        "id": "QbbbDvbU8bW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_fn = nn.BCELoss() # Requires sigmoid on input\n",
        "loss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input\n",
        "optimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "TGjiLQAPq1Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "epochs = 1000 # Train for longer\n",
        "\n",
        "# Put data to target device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    # 1. Forward pass\n",
        "    y_logits = model_1(X_train).squeeze()\n",
        "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels\n",
        "\n",
        "    # 2. Calculate loss/accuracy\n",
        "    loss = loss_fn(y_logits, y_train)\n",
        "    acc = accuracy_fn(y_true=y_train,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    model_1.eval()\n",
        "    with torch.inference_mode():\n",
        "        # 1. Forward pass\n",
        "        test_logits = model_1(X_test).squeeze()\n",
        "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "        # 2. Caculate loss/accuracy\n",
        "        test_loss = loss_fn(test_logits,\n",
        "                            y_test)\n",
        "        test_acc = accuracy_fn(y_true=y_test,\n",
        "                               y_pred=test_pred)\n",
        "\n",
        "    # Print out what's happening every 10 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "VuwwyiLDq6dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundaries for training and test sets\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_1, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_1, X_test, y_test)"
      ],
      "metadata": {
        "id": "3Hyy9mFUq-bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirming the model works for training"
      ],
      "metadata": {
        "id": "aaRcjTDZrIy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some data (same as notebook 01)\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.01\n",
        "\n",
        "# Create data\n",
        "X_regression = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y_regression = weight * X_regression + bias # linear regression formula\n",
        "\n",
        "# Check the data\n",
        "print(len(X_regression))\n",
        "X_regression[:5], y_regression[:5]\n",
        "\n",
        "# Create train and test splits\n",
        "train_split = int(0.8 * len(X_regression)) # 80% of data used for training set\n",
        "X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\n",
        "X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n",
        "\n",
        "# Check the lengths of each split\n",
        "print(len(X_train_regression),\n",
        "    len(y_train_regression),\n",
        "    len(X_test_regression),\n",
        "    len(y_test_regression))\n",
        "\n",
        "\n",
        "plot_predictions(train_data=X_train_regression,\n",
        "    train_labels=y_train_regression,\n",
        "    test_data=X_test_regression,\n",
        "    test_labels=y_test_regression\n",
        ");"
      ],
      "metadata": {
        "id": "N_kNUbdJrIIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same architecture as model_1 (but using nn.Sequential)\n",
        "model_2 = nn.Sequential(\n",
        "    nn.Linear(in_features=1, out_features=10),\n",
        "    nn.Linear(in_features=10, out_features=10),\n",
        "    nn.Linear(in_features=10, out_features=1)\n",
        ").to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = nn.L1Loss()\n",
        "optimizer = torch.optim.SGD(model_2.parameters(), lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 1000\n",
        "\n",
        "# Put data to target device\n",
        "X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\n",
        "X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    # 1. Forward pass\n",
        "    y_pred = model_2(X_train_regression)\n",
        "\n",
        "    # 2. Calculate loss (no accuracy since it's a regression problem, not classification)\n",
        "    loss = loss_fn(y_pred, y_train_regression)\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    model_2.eval()\n",
        "    with torch.inference_mode():\n",
        "      # 1. Forward pass\n",
        "      test_pred = model_2(X_test_regression)\n",
        "      # 2. Calculate the loss\n",
        "      test_loss = loss_fn(test_pred, y_test_regression)\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}\")\n",
        "\n",
        "# Turn on evaluation mode\n",
        "model_2.eval()\n",
        "\n",
        "# Make predictions (inference)\n",
        "with torch.inference_mode():\n",
        "    y_preds = model_2(X_test_regression)\n",
        "\n",
        "# Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)\n",
        "# (try removing .cpu() from one of the below and see what happens)\n",
        "plot_predictions(train_data=X_train_regression.cpu(),\n",
        "                 train_labels=y_train_regression.cpu(),\n",
        "                 test_data=X_test_regression.cpu(),\n",
        "                 test_labels=y_test_regression.cpu(),\n",
        "                 predictions=y_preds.cpu());"
      ],
      "metadata": {
        "id": "rLqPX5TOrmZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction of non linearity"
      ],
      "metadata": {
        "id": "Gt62ebfQsJJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and plot data\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "n_samples = 1000\n",
        "\n",
        "X, y = make_circles(n_samples=1000,\n",
        "    noise=0.03,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y = torch.from_numpy(y).type(torch.float)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42\n",
        ")\n",
        "\n",
        "X_train[:5], y_train[:5]"
      ],
      "metadata": {
        "id": "XZPa9qq7sgLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model with non-linear activation function\n",
        "from torch import nn\n",
        "class CircleModelV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
        "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
        "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
        "        self.relu = nn.ReLU() # <- add in ReLU activation function\n",
        "        # Can also put sigmoid in the model\n",
        "        # This would mean you don't need to use it on the predictions\n",
        "        # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Intersperse the ReLU activation function between layers\n",
        "       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
        "\n",
        "model_3 = CircleModelV2().to(device)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.SGD(model_3.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "NHD2Hh-Ls8Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "torch.manual_seed(42)\n",
        "epochs = 10000\n",
        "\n",
        "# Put all data on target device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # 1. Forward pass\n",
        "    y_logits = model_3(X_train).squeeze()\n",
        "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels\n",
        "\n",
        "    # 2. Calculate loss and accuracy\n",
        "    loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n",
        "    acc = accuracy_fn(y_true=y_train,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    model_3.eval()\n",
        "    with torch.inference_mode():\n",
        "      # 1. Forward pass\n",
        "      test_logits = model_3(X_test).squeeze()\n",
        "      test_pred = torch.round(torch.sigmoid(test_logits)) # logits -> prediction probabilities -> prediction labels\n",
        "      # 2. Calculate loss and accuracy\n",
        "      test_loss = loss_fn(test_logits, y_test)\n",
        "      test_acc = accuracy_fn(y_true=y_test,\n",
        "                             y_pred=test_pred)\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "dHkdc1VitfW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "model_3.eval()\n",
        "with torch.inference_mode():\n",
        "    y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\n",
        "y_preds[:10], y[:10] # want preds in same format as truth labels"
      ],
      "metadata": {
        "id": "FAZ0KxTzt_CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundaries for training and test sets\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity"
      ],
      "metadata": {
        "id": "vq2IDA2YuAa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replicating Non Linear Activation Fns"
      ],
      "metadata": {
        "id": "hs8JjCBoulLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU\n",
        "A = torch.arange(-10, 10, 1, dtype=torch.float32)\n",
        "def relu(x):\n",
        "  return torch.maximum(torch.tensor(0), x)\n",
        "relu(A)\n",
        "plt.plot(relu(A));"
      ],
      "metadata": {
        "id": "umkLUL-vukrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "# Test custom sigmoid on toy tensor\n",
        "sigmoid(A)\n",
        "plt.plot(sigmoid(A));"
      ],
      "metadata": {
        "id": "pRfAHvw-vE3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi class Pytorch Model\n",
        "\n"
      ],
      "metadata": {
        "id": "-wKwSMEFvXQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the hyperparameters for data creation\n",
        "NUM_CLASSES = 4\n",
        "NUM_FEATURES = 2\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# 1. Create multi-class data\n",
        "X_blob, y_blob = make_blobs(n_samples=1000,\n",
        "    n_features=NUM_FEATURES, # X features\n",
        "    centers=NUM_CLASSES, # y labels\n",
        "    cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# 2. Turn data into tensors\n",
        "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
        "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n",
        "print(X_blob[:5], y_blob[:5])\n",
        "\n",
        "# 3. Split into train and test sets\n",
        "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n",
        "    y_blob,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# 4. Plot data\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);"
      ],
      "metadata": {
        "id": "xX6y7VbuvbKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "# Build model\n",
        "class BlobModel(nn.Module):\n",
        "    def __init__(self, input_features, output_features, hidden_units=8):\n",
        "        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n",
        "\n",
        "        Args:\n",
        "            input_features (int): Number of input features to the model.\n",
        "            out_features (int): Number of output features of the model\n",
        "              (how many classes there are).\n",
        "            hidden_units (int): Number of hidden units between layers, default 8.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear_layer_stack = nn.Sequential(\n",
        "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer_stack(x)\n",
        "\n",
        "# Create an instance of BlobModel and send it to the target device\n",
        "model_4 = BlobModel(input_features=NUM_FEATURES,\n",
        "                    output_features=NUM_CLASSES,\n",
        "                    hidden_units=8).to(device)\n",
        "\n",
        "\n",
        "# Create loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_4.parameters(),\n",
        "                            lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance"
      ],
      "metadata": {
        "id": "kMqOPLP2wG36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a single forward pass on the data (we'll need to put it to the target device for it to work)\n",
        "model_4(X_blob_train.to(device))[:5]"
      ],
      "metadata": {
        "id": "D9VEvIW0waEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "epochs = 100\n",
        "\n",
        "# Put data to target device\n",
        "X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\n",
        "X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    model_4.train()\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_logits = model_4(X_blob_train) # model outputs raw logits\n",
        "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
        "    # print(y_logits)\n",
        "    # 2. Calculate loss and accuracy\n",
        "    loss = loss_fn(y_logits, y_blob_train)\n",
        "    acc = accuracy_fn(y_true=y_blob_train,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    model_4.eval()\n",
        "    with torch.inference_mode():\n",
        "      # 1. Forward pass\n",
        "      test_logits = model_4(X_blob_test)\n",
        "      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
        "      # 2. Calculate test loss and accuracy\n",
        "      test_loss = loss_fn(test_logits, y_blob_test)\n",
        "      test_acc = accuracy_fn(y_true=y_blob_test,\n",
        "                             y_pred=test_pred)\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "Bsv5yGalxRV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.eval()\n",
        "with torch.inference_mode():\n",
        "    y_logits = model_4(X_blob_test)\n",
        "\n",
        "# View the first 10 predictions\n",
        "y_logits[:10]\n",
        "\n",
        "# Turn predicted logits in prediction probabilities\n",
        "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
        "\n",
        "# Turn prediction probabilities into prediction labels\n",
        "y_preds = y_pred_probs.argmax(dim=1)\n",
        "\n",
        "# Compare first 10 model preds and test labels\n",
        "print(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\")\n",
        "print(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_4, X_blob_train, y_blob_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_4, X_blob_test, y_blob_test)"
      ],
      "metadata": {
        "id": "JrzKNZmXybjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from torchmetrics import Accuracy\n",
        "except:\n",
        "    !pip install torchmetrics==0.9.3 # this is the version we're using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)\n",
        "    from torchmetrics import Accuracy\n",
        "\n",
        "# Setup metric and make sure it's on the target device\n",
        "torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=2).to(device) # Corrected num_classes to 2\n",
        "\n",
        "# Calculate accuracy\n",
        "torchmetrics_accuracy(y_preds_moon, y_moon_test)"
      ],
      "metadata": {
        "id": "lds7KTZVzL72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### make moons exercise"
      ],
      "metadata": {
        "id": "s1xS5QeEzpnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Set the hyperparameters for data creation\n",
        "NUM_FEATURES = 2\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
        "\n",
        "X_moons = torch.from_numpy(X).type(torch.float)\n",
        "y_moons = torch.from_numpy(y).type(torch.LongTensor)\n",
        "\n",
        "X_moon_train, X_moon_test, y_moon_train, y_moon_test = train_test_split(X_moons,\n",
        "    y_moons,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# 4. Plot data\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(X_moon_train[:, 0], X_moon_train[:, 1], c=y_moon_train, cmap=plt.cm.RdYlBu);"
      ],
      "metadata": {
        "id": "9jhe23WBzsy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class MoonModelV0(nn.Module):\n",
        "    def __init__(self, in_features, out_features, hidden_units):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(in_features=in_features,\n",
        "                                 out_features=hidden_units)\n",
        "        self.layer2 = nn.Linear(in_features=hidden_units,\n",
        "                                 out_features=hidden_units)\n",
        "        self.layer3 = nn.Linear(in_features=hidden_units,\n",
        "                                out_features=out_features)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer3(self.relu(self.layer2(self.relu(self.layer1(x)))))\n",
        "\n",
        "model_0 = MoonModelV0(in_features=2,\n",
        "                      out_features=1,\n",
        "                      hidden_units=10).to(device)\n",
        "model_0"
      ],
      "metadata": {
        "id": "40HP3aWp6g4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of model to optimize\n",
        "                            lr=0.1) # learning rate"
      ],
      "metadata": {
        "id": "tukVqF-x6n8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import Accuracy\n",
        "acc_fn = Accuracy(task=\"binary\").to(device)"
      ],
      "metadata": {
        "id": "tSBkNm8E7EDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "epochs=1000\n",
        "\n",
        "# Send data to the device\n",
        "X_train, y_train = X_moon_train.to(device), y_moon_train.to(device)\n",
        "X_test, y_test = X_moon_test.to(device), y_moon_test.to(device)\n",
        "\n",
        "# Loop through the data\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_0.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_logits = model_0(X_train).squeeze()\n",
        "  # print(y_logits[:5]) # model raw outputs are \"logits\"\n",
        "  y_pred_probs = torch.sigmoid(y_logits)\n",
        "  y_pred = torch.round(y_pred_probs)\n",
        "\n",
        "  # 2. Calculaute the loss\n",
        "  loss = loss_fn(y_logits, y_train) # loss = compare model raw outputs to desired model outputs\n",
        "  acc = acc_fn(y_pred, y_train.int()) # the accuracy function needs to compare pred labels (not logits) with actual labels\n",
        "\n",
        "  # 3. Zero the gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Loss backward (perform backpropagation) - https://brilliant.org/wiki/backpropagation/#:~:text=Backpropagation%2C%20short%20for%20%22backward%20propagation,to%20the%20neural%20network's%20weights.\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Step the optimizer (gradient descent) - https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21#:~:text=Gradient%20descent%20(GD)%20is%20an,e.g.%20in%20a%20linear%20regression)\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    # 1. Forward pass\n",
        "    test_logits = model_0(X_test).squeeze()\n",
        "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "    # 2. Caculate the loss/acc\n",
        "    test_loss = loss_fn(test_logits, y_test)\n",
        "    test_acc = acc_fn(test_pred, y_test.int())\n",
        "\n",
        "  # Print out what's happening\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss:.2f} Acc: {acc:.2f} | Test loss: {test_loss:.2f} Test acc: {test_acc:.2f}\")"
      ],
      "metadata": {
        "id": "g9lwWNRT7Ilq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# TK - this could go in the helper_functions.py and be explained there\n",
        "def plot_decision_boundary(model, X, y):\n",
        "\n",
        "    # Put everything to CPU (works better with NumPy + Matplotlib)\n",
        "    model.to(\"cpu\")\n",
        "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
        "\n",
        "    # Source - https://madewithml.com/courses/foundations/neural-networks/\n",
        "    # (with modifications)\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101),\n",
        "                         np.linspace(y_min, y_max, 101))\n",
        "\n",
        "    # Make features\n",
        "    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n",
        "\n",
        "    # Make predictions\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        y_logits = model(X_to_pred_on)\n",
        "\n",
        "    # Test for multi-class or binary and adjust logits to prediction labels\n",
        "    if len(torch.unique(y)) > 2:\n",
        "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # mutli-class\n",
        "    else:\n",
        "        y_pred = torch.round(torch.sigmoid(y_logits)) # binary\n",
        "\n",
        "    # Reshape preds and plot\n",
        "    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n",
        "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "\n",
        "# Plot decision boundaries for training and test sets\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_0, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_0, X_test, y_test)"
      ],
      "metadata": {
        "id": "HDOOZa85-YXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # PyTorch Computer Vision\n"
      ],
      "metadata": {
        "id": "-2QfrO9V-_e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to?\n",
        "    train=True, # get training data\n",
        "    download=True, # download data if it doesn't exist on disk\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
        "    target_transform=None # you can transform labels as well\n",
        ")\n",
        "\n",
        "# Setup testing data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "zJ5pkjKv_GQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "class_names = train_data.classes\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "rows, cols = 4, 4\n",
        "for i in range(1, rows * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(rows, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False);"
      ],
      "metadata": {
        "id": "XObtrM3GBkW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
        "    batch_size=BATCH_SIZE, # how many samples per batch?\n",
        "    shuffle=True # shuffle data every epoch?\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False # don't necessarily have to shuffle the testing data\n",
        ")\n",
        "\n",
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n",
        "\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "id": "T-JDcYYqEAzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
        "\n",
        "# Try uncommenting below and see what happens\n",
        "#print(x)\n",
        "#print(output)"
      ],
      "metadata": {
        "id": "NYV2b7biEb_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
        "\n",
        "# Try uncommenting below and see what happens\n",
        "# print(x)\n",
        "# print(output)"
      ],
      "metadata": {
        "id": "hHyHQ9RsEtq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class FashionMNISTModelV0(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(), # neural networks like their inputs in vector form\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer_stack(x)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Need to setup model with input parameters\n",
        "model_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n",
        "    hidden_units=10, # how many units in the hidden layer\n",
        "    output_shape=len(class_names) # one for every class\n",
        ")\n",
        "model_0.to(\"cpu\") # keep model on CPU to begin with"
      ],
      "metadata": {
        "id": "1pDU_2RyF2mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "NmiP7ebpGpuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "m1v-f3sSGv_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "7Ti3U51CG_Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the seed and start the timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set the number of epochs (we'll keep this small for faster training times)\n",
        "epochs = 3\n",
        "\n",
        "# Create training and testing loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    ### Training\n",
        "    train_loss = 0\n",
        "    # Add a loop to loop through training batches\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        model_0.train()\n",
        "        # 1. Forward pass\n",
        "        y_pred = model_0(X)\n",
        "\n",
        "        # 2. Calculate loss (per batch)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss # accumulatively add up the loss per epoch\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print out how many samples have been seen\n",
        "        if batch % 400 == 0:\n",
        "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    ### Testing\n",
        "    # Setup variables for accumulatively adding up loss and accuracy\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in test_dataloader:\n",
        "            # 1. Forward pass\n",
        "            test_pred = model_0(X)\n",
        "\n",
        "            # 2. Calculate loss (accumulatively)\n",
        "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
        "        # Divide total test loss by length of test dataloader (per batch)\n",
        "        test_loss /= len(test_dataloader)\n",
        "\n",
        "        # Divide total accuracy by length of test dataloader (per batch)\n",
        "        test_acc /= len(test_dataloader)\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
        "\n",
        "# Calculate training time\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
        "                                           end=train_time_end_on_cpu,\n",
        "                                           device=str(next(model_0.parameters()).device))"
      ],
      "metadata": {
        "id": "DXzBSabAJ-7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn):\n",
        "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Make predictions with the model\n",
        "            y_pred = model(X)\n",
        "\n",
        "            # Accumulate the loss and accuracy values per batch\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y,\n",
        "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
        "\n",
        "        # Scale loss and acc to find the average loss/acc per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 0 results on test dataset\n",
        "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
        ")\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "DeKwvU0XKoa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "mNjlTgpKKqRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model with non-linear and linear layers\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(), # flatten inputs into single vector\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "6wrugT_oKx6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names) # number of output classes desired\n",
        ").to(device) # send model to GPU if it's available\n",
        "next(model_1.parameters()).device # check model device"
      ],
      "metadata": {
        "id": "-yasqDrPLSxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
        "                            lr=0.1)"
      ],
      "metadata": {
        "id": "z__yGO_RLWE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    model.to(device)\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Send data to GPU\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate loss and accuracy per epoch and print out what's happening\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "def test_step(data_loader: torch.utils.data.DataLoader,\n",
        "              model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.to(device)\n",
        "    model.eval() # put model in eval mode\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to GPU\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # 2. Calculate loss and accuracy\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
        "            )\n",
        "\n",
        "        # Adjust metrics and print out\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "9gc0p3MILw1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader,\n",
        "        model=model_1,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=model_1,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "\n",
        "train_time_end_on_gpu = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
        "                                            end=train_time_end_on_gpu,\n",
        "                                            device=device)"
      ],
      "metadata": {
        "id": "7JjUDVKUL_BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Note: This will error due to `eval_model()` not using device agnostic code\n",
        "model_1_results = eval_model(model=model_1,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "brW5bPNMMSEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move values to device\n",
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    \"\"\"Evaluates a given model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "        device (str, optional): Target device to compute on. Defaults to device.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to the target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # Scale loss and acc\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 1 results with device-agnostic code\n",
        "model_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n",
        "    device=device\n",
        ")\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "6Dl22itQMt0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results"
      ],
      "metadata": {
        "id": "1PwC8fxzM0SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a CNN\n",
        "\n",
        "Input layer -> [Convolutional layer -> activation layer -> pooling layer] -> Output layer\n",
        "\n",
        "- Structured Data - use gradient boosted models, random forest, XGBoost\n",
        "- Unstructured Data - cnns, transformers\n",
        "\n",
        "\n",
        "Convolutional operation - apply a filter  "
      ],
      "metadata": {
        "id": "iY4isM2PNBsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a convolutional neural network\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because each layer of our network compresses and changes the shape of our input data.\n",
        "            nn.Linear(in_features=hidden_units*7*7,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_2 = FashionMNISTModelV2(input_shape=1,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)).to(device)\n",
        "model_2"
      ],
      "metadata": {
        "id": "OvpimFLxNLah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create sample batch of random numbers with same size as image batch\n",
        "images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n",
        "test_image = images[0]\n",
        "\n",
        "conv_layer = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=10,\n",
        "                       kernel_size=3,\n",
        "                       stride=1,\n",
        "                       padding=0) # also try using \"valid\" or \"same\" here\n",
        "test_image.unsqueeze(dim=0).shape\n",
        "conv_layer(test_image.unsqueeze(dim=0)).shape"
      ],
      "metadata": {
        "id": "mRzk0jx-Piy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.conv2d:\n",
        "\n",
        "Convolutions in CNNs serve one core purpose: detecting local patterns regardless of where they appear in an image.\n",
        "Think of it like sliding a small \"pattern detector\" across the entire image. A 33 filter might learn to detect a vertical edge, a corner, or a specific texture. Because the same filter scans everywhere, the network can recognize that pattern whether it's in the top-left or bottom-rightthis is translation invariance.\n",
        "The key advantages:\n",
        "Parameter efficiency  Instead of connecting every pixel to every neuron (millions of parameters), you reuse the same small filter everywhere. A 33 filter has just 9 weights, no matter how large your image.\n",
        "Hierarchical feature learning  Early layers detect simple things (edges, colors). Deeper layers combine those into complex patterns (eyes, wheels, faces). Each layer builds on the spatial relationships found in the previous one.\n",
        "Preserving spatial structure  Unlike flattening an image into a vector, convolutions maintain the 2D arrangement, so the network understands that neighboring pixels are related."
      ],
      "metadata": {
        "id": "zrWsfGkrUBLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out original image shape without and with unsqueezed dimension\n",
        "print(f\"Test image original shape: {test_image.shape}\")\n",
        "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n",
        "\n",
        "# Create a sample nn.MaxPoo2d() layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass data through just the conv_layer\n",
        "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
        "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
        "\n",
        "# Pass data through the max pool layer\n",
        "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
        "print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")"
      ],
      "metadata": {
        "id": "fSqGgkTOUTwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a random tensor with a similar number of dimensions to our images\n",
        "random_tensor = torch.randn(size=(1, 1, 2, 2))\n",
        "print(f\"Random tensor:\\n{random_tensor}\")\n",
        "print(f\"Random tensor shape: {random_tensor.shape}\")\n",
        "\n",
        "# Create a max pool layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value\n",
        "\n",
        "# Pass the random tensor through the max pool layer\n",
        "max_pool_tensor = max_pool_layer(random_tensor)\n",
        "print(f\"\\nMax pool tensor:\\n{max_pool_tensor} <- this is the maximum value from random_tensor\")\n",
        "print(f\"Max pool tensor shape: {max_pool_tensor.shape}\")"
      ],
      "metadata": {
        "id": "A0PD1Pj4XCyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every layer in a neural network is trying to compress data from a higher dimensional space to lower dimensional space.\n",
        "\n",
        "The whole goal can be considered to compress information\n"
      ],
      "metadata": {
        "id": "crTb3WUSXbtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(),\n",
        "                             lr=0.1)"
      ],
      "metadata": {
        "id": "VlG-ObHwa1mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_model_2 = timer()\n",
        "\n",
        "# Train and test model\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader,\n",
        "        model=model_2,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=model_2,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "train_time_end_model_2 = timer()\n",
        "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
        "                                           end=train_time_end_model_2,\n",
        "                                           device=device)"
      ],
      "metadata": {
        "id": "lmr1LxqGdhtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "Used to evaluate classification models"
      ],
      "metadata": {
        "id": "yVgI732RePKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Make predictions with trained model\n",
        "y_preds = []\n",
        "model_2.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Do the forward pass\n",
        "    y_logit = model_2(X)\n",
        "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
        "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)"
      ],
      "metadata": {
        "id": "Gz4eBdKveR82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# See if torchmetrics exists, if not, install it\n",
        "try:\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n",
        "except:\n",
        "    print(\"Installing compatible numpy, torchmetrics, and mlxtend...\")\n",
        "    # Downgrade numpy to a compatible version\n",
        "    !pip install numpy==1.25.2 # This version is generally compatible with older scipy/torchmetrics\n",
        "    !pip install -q torchmetrics==0.9.3 mlxtend # Re-install torchmetrics and mlxtend\n",
        "    print(\"Restarting runtime is recommended after this installation.\")\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "\n",
        "# It's crucial to restart the runtime after downgrading numpy.\n",
        "# If the error persists after running this cell, please restart the runtime manually via Runtime -> Restart runtime."
      ],
      "metadata": {
        "id": "1Ka7D0cDe5TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZm0XY_MfU1X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}